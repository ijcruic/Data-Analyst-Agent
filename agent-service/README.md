# Data Analyst Agent Service

FastAPI microservice that exposes the LangGraph-powered data analyst agent used by the web UI. The service proxies user analysis requests to the Model Context Protocol (MCP) backend and returns textual answers, optional plots, and downloadable artifacts.

## Prerequisites

- Python dependencies from `requirements.txt`
- Access credentials for the configured LLM provider (defaults to OpenAI-compatible APIs)
- Reachable MCP server endpoint(s) that provide data analysis tools

## Quick Start

```bash
uvicorn main:app --host 0.0.0.0 --port 8002 --reload
```

By default the service loads configuration from `config.yaml`. Override values with environment variables or a different config file as needed.

## Architecture

```mermaid
flowchart LR
    UI[Web UI / API client] -->|POST /analyze| FastAPI[FastAPI agent-service]
    FastAPI --> Agent[LangGraph ReAct agent]
    Agent -->|LLM turn| LLM[ChatOpenAIVLLM / ChatBedrock]
    Agent -->|ToolNode| MCPTools[MCP ToolNode]
    MCPTools --> MCP[MCP server(s)]
    MCP --> Agent
    Agent -->|Artifacts| Cache[(Local artifact cache)]
    Agent --> FastAPI
```

1. **FastAPI layer** receives requests and hands them to `DataAnalystAgent.analyze`.
2. **LangGraph ReAct agent** alternates between the LLM and the `ToolNode` loaded from MCP servers.
3. **LLM node** uses `ChatOpenAIVLLM` for OpenAI-compatible endpoints or `ChatBedrock` when AWS Bedrock is selected.
4. **Tool execution** happens through the MCP adapter; any outputs are cached under `/tmp/agent_artifacts` so the web UI can stream them back.
5. **Tool-call recovery**: when the LLM mentions a tool but fails to emit a structured call, an explicit feedback node nudges it to retry instead of returning partial text to the user.

## Configuration

| Section              | Key                                     | Description |
|----------------------|-----------------------------------------|-------------|
| `server`             | `host`, `port`                          | Network interface and port for Uvicorn |
| `llm`                | `provider`, `api_base`, `model`, `temperature`, `api_key`, `aws.*` | LLM settings (OpenAI-compatible by default, or AWS Bedrock when `provider` is set to `bedrock`) |
| `mcp`                | `endpoints`, `connect_timeout_seconds`, `max_retries` | MCP discovery and retry behaviour |
| `storage.minio`      | `endpoint`, `bucket`, credentials, `secure` | Artifact storage the MCP tools can leverage |
| `telemetry.matomo`   | `site_id`, `url`, `ssl_verify`          | Optional analytics configuration |
| `logging`            | `level`, `json`                         | Log verbosity and format |

## Agent Behaviour

- Every conversation starts with a system prompt that enumerates the core MCP tools and reminds the model to include `session_id` in every call.
- The assistant node inspects raw LLM text for tool-like patterns; if a structured call is missing, it injects a feedback message prompting a retry.
- Tool failures (validation errors, network issues, etc.) are surfaced back to the model as `ToolMessage` content along with corrective guidance.
- Published artifacts are materialised locally and exposed via `/artifacts/{session_id}/{artifact_id}` for download in the UI.

## API

All routes expect and return JSON unless otherwise noted.

### `GET /health`

Simple readiness check. Returns:

```json
{ "status": "healthy" }
```

### `POST /analyze`

Kick off a natural-language analysis request.

- **Request body**

```json
{
  "query": "Summarize the latest sales trends and plot monthly totals.",
  "session_id": "user-123"
}
```

Use a stable `session_id` per user conversation so cached artifacts and context can be reused.

- **Response body**

```json
{
  "text": "Summary of findings …",
  "plot_base64": "data:image/png;base64,…",
  "result_id": "c8a1b2e4",
  "artifacts": [
    {
      "artifact_id": "table-1",
      "description": "Detailed metrics CSV",
      "proxy_path": "/artifacts/user-123/table-1",
      "mime_type": "text/csv"
    }
  ]
}
```

`plot_base64` may be omitted when no visualization is produced. `artifacts` provides download metadata for files generated by MCP tools.

### `GET /artifacts/{session_id}/{artifact_id}`

Downloads a previously generated artifact for the provided session and artifact identifiers. Returns the file as an attachment with the appropriate MIME type.

## Switching LLM Providers

The agent reads provider settings from the `llm` section of `config.yaml`, with environment variables taking precedence.

- **OpenAI / vLLM (default)**  
  Leave `llm.provider` as `openai` and supply `OPENAI_API_KEY` (or `AGENT_LLM_API_KEY`). Override `llm.api_base` to target a self-hosted OpenAI-compatible endpoint such as vLLM.

- **AWS Bedrock**  
  Set `llm.provider` to `bedrock`. Provide Bedrock connection details via environment variables or your orchestrator:
  - `AGENT_LLM_AWS_REGION` / `AWS_REGION`
  - Optional: `AGENT_LLM_AWS_PROFILE` / `AWS_PROFILE`
  - Optional: `AGENT_LLM_AWS_ACCESS_KEY_ID`, `AGENT_LLM_AWS_SECRET_ACCESS_KEY`, `AGENT_LLM_AWS_SESSION_TOKEN` (omit when using IAM roles)
  - Optional: `AGENT_LLM_AWS_ENDPOINT_URL` for private Bedrock endpoints

  The configured `llm.model` becomes the Bedrock model ID (e.g. `anthropic.claude-3-sonnet-20240229-v1:0`). No code changes are required when switching environments—update only your deployment config or Kubernetes manifests.

## Deployment Notes

- The Dockerfile installs `pandoc` to render this README into `README.html` for serving to end users.
- Ensure `config.yaml` is baked into the image or mounted at runtime with the correct secrets (`llm.api_key`, storage credentials, etc.).
- When running behind a proxy or orchestrator, expose port `8002` (or the configured `server.port`).
